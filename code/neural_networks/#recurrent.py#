import copy
import numpy as np

# compute sigmoid nonlinearity
def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output

# convert output of sigmoid function to its derivative
def sigmoid_output_to_derivative(output):
    return output*(1-output)

def create_connection(num_rows,num_cols):
    return 2*np.random.random((num_rows,num_cols)) -1

def create_nn(input_data,output_data,depth_hidden_layers,breathe_hidden_layers):
    nn = [{"name":"input data","connection":input_data}]
    #input layer
    input_syn = {"name":"input layer"}
    input_syn["connection"] = create_connection(len(input_data[0]),breathe_hidden_layers)
    nn.append(input_syn)
    #hidden layers
    for i in xrange(depth_hidden_layers):
        syn = {"name":i}
        syn["connection"] = create_connection(breathe_hidden_layers,breathe_hidden_layers)
        nn.append(syn)
    #output_layer
    syn = {"name":"output layer"}
    syn["connection"] = create_connection(breathe_hidden_layers,len(output_data[0]))
    nn.append(syn)
    nn.append({"name":"output data","connection":output_data})
    return nn

def forward_propagate(synapses):
    layers = [synapses[0]["connection"]]
    for ind,synapse in enumerate(synapses[:-1]):
        if ind == 0: continue
        layers.append(
            nonlin(np.dot(layers[ind-1],synapse["connection"]))
        )
    return layers

def back_propagate(layers,synapses,alpha=0.1):
    errors = [synapses[-1]["connection"] - layers[-1]]
    synapses_index = -1
    layers_index = -1
    errors_index = 0
    deltas_index = 0
    deltas = []
    while len(layers) - abs(layers_index) > 0:
        deltas.append(errors[errors_index]*nonlin(layers[layers_index],deriv=True))
        synapses_index -= 1
        layers_index -= 1
        errors.append(deltas[deltas_index].dot(synapses[synapses_index]["connection"].T))
        errors_index += 1
        deltas_index += 1
    synapses_index = -2
    layers_index = -2
    deltas_index = 0
    while len(layers) - abs(layers_index) >= 0:
        synapses[synapses_index]["connection"] += alpha * layers[layers_index].T.dot(deltas[deltas_index])
        synapses_index -= 1
        layers_index -= 1
        deltas_index += 1
    return synapses,errors[0]

def train_network(X,y,depth,breathe,alpha=0.1,num_iterations=70000,dropout=False,dropout_percent=0.25):
    errors = []
    nn = create_nn(X,y,depth,breathe)
    for j in xrange(num_iterations):
        layers = forward_propagate(nn)
        if dropout:
            layers = apply_dropout(nn,layers,X,breathe,dropout_percent) 
        nn,error = back_propagate(layers,nn,alpha=alpha)
        if j %1000 == 0:   
            errors.append(np.mean(np.abs(error)))
    return errors

def integer2binary(largest_number=8):
    i2b = {}
    binary_dim = pow(2,largest_number)
    binary = np.unpackbits(
        np.array([range(binary_dim)],dtype=np.uint8).T,axis=1)
    for i in xrange(binary_dim):
        i2b[i] = binary[i]
    return i2b
# training dataset generation

def initialize_update(synapses):
    updates = []
    for synapse in synapses:
        updates.append(np.zeros_like(synapse))
    return updates
    
int2binary = integer2binary()
# input variables
alpha = 0.1
input_dim = 2
hidden_dim = 16
output_dim = 1

# initialize neural network weights
synapse_0 = create_connection(input_dim,hidden_dim)
synapse_1 = create_connection(hidden_dim,output_dim)
synapse_h = create_connection(hidden_dim,hidden_dim)

synapse_0_update = np.zeros_like(synapse_0)
synapse_1_update = np.zeros_like(synapse_1)
synapse_h_update = np.zeros_like(synapse_h)

def generate_training(int2binary):
    integer_training_data = []
    binary_training_data = []
    for j in range(10000):

        tmp_int = {}
        tmp_bin = {}
        # generate a simple addition problem (a + b = c)
        tmp_int["a"] = np.random.randint(largest_number/2) # int version
        tmp_bin["a"] = int2binary[a_int] # binary encoding
        
        tmp_int["b"] = np.random.randint(largest_number/2) # int version
        tmp_bin["b"] = int2binary[b_int] # binary encoding
        
        # true answer
        tmp_int["c"] = tmp_int["a"] + tmp_int["b"]
        tmp_bin["c"] = int2binary[ tmp_int["c"] ] 
        integer_training_data.append(tmp_int)
        binary_training_data.append(tmp_bin)
    return integer_training_data,binary_training_data

# training logic
integer_training_data,binary_training_data = generate_training_data(int2binary)
for ind,val in enumerate(integer_training_data):
    # where we'll store our best guess (binary encoded)
    d = np.zeros_like(binary_training_data[ind]["c"])
    
    overallError = 0
    
    layer_2_deltas = list()
    layer_1_values = list()
    layer_1_values.append(np.zeros(hidden_dim))
    
    # moving along the positions in the binary encoding
    for position in range(binary_dim):
        
        # generate input and output
        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])
        y = np.array([[c[binary_dim - position - 1]]]).T

        # hidden layer (input ~+ prev_hidden)
        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))

        # output layer (new binary representation)
        layer_2 = sigmoid(np.dot(layer_1,synapse_1))

        # did we miss?... if so, by how much?
        layer_2_error = y - layer_2
        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))
        overallError += np.abs(layer_2_error[0])
    
        # decode estimate so we can print it out
        d[binary_dim - position - 1] = np.round(layer_2[0][0])
        
        # store hidden layer so we can use it in the next timestep
        layer_1_values.append(copy.deepcopy(layer_1))
    
    future_layer_1_delta = np.zeros(hidden_dim)
    
    for position in range(binary_dim):
        
        X = np.array([[a[position],b[position]]])
        layer_1 = layer_1_values[-position-1]
        prev_layer_1 = layer_1_values[-position-2]
        
        # error at output layer
        layer_2_delta = layer_2_deltas[-position-1]
        # error at hidden layer
        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)

        # let's update all our weights so we can try again
        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)
        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)
        synapse_0_update += X.T.dot(layer_1_delta)
        
        future_layer_1_delta = layer_1_delta
    

    synapse_0 += synapse_0_update * alpha
    synapse_1 += synapse_1_update * alpha
    synapse_h += synapse_h_update * alpha    

    synapse_0_update *= 0
    synapse_1_update *= 0
    synapse_h_update *= 0
    
    # print out progress
    if(j % 1000 == 0):
        print "Error:" + str(overallError)
        print "Pred:" + str(d)
        print "True:" + str(c)
        out = 0
        for index,x in enumerate(reversed(d)):
            out += x*pow(2,index)
        print str(a_int) + " + " + str(b_int) + " = " + str(out)
        print "------------"

        
